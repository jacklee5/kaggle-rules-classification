{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fc5cbc3",
   "metadata": {},
   "source": [
    "## Qwen2.5-LoRA-Finetune-Baseline-Train\n",
    "This notebook demonstrates how to fine-tune a large language model (Qwen2.5) using the LoRA (Low-Rank Adaptation) technique for classification tasks. The example uses the Jigsaw dataset to classify if Reddit comments violate community rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42538eac",
   "metadata": {},
   "source": [
    "### Note\n",
    "The training code is for demonstration purposes only. To reproduce the weights in the inference notebook, you need to change the config as follows. Training was performed on a local A6000.\n",
    "\n",
    "- `IS_DEBUG = True` → `False`\n",
    "- `MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4\"` → `\"Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4\"`\n",
    "- `TRAIN_BS = 1` → `8`\n",
    "- `GRAD_ACC_NUM = 8` → `1`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd93151",
   "metadata": {},
   "source": [
    "### References\n",
    "* https://www.kaggle.com/code/abdmental01/jigsaw-mpnet-base-v2-inference-cv-0-876\n",
    "* https://www.kaggle.com/code/aerdem4/jigsaw-acrc-qwen7b-finetune-logits-processor-zoo\n",
    "* https://www.guruguru.science/competitions/24/discussions/21027ff1-2074-4e21-a249-b2d4170bd516/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ccc56a",
   "metadata": {},
   "source": [
    "### 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9645491c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Kaggle\\rules-classification\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "import wandb\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers.utils import is_torch_bf16_gpu_available\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from trl import DataCollatorForCompletionOnlyLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce16c5d4",
   "metadata": {},
   "source": [
    "### 2. Configuration Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d922511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main configuration parameters\n",
    "ENV = \"local\"  # \"local\" or \"kaggle\"\n",
    "\n",
    "WANDB = False  # Enable/disable Weights & Biases logging\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4\"  # Pre-trained model to fine-tune\n",
    "IS_DEBUG = True  # Debug mode with small dataset\n",
    "N_FOLDS = 5  # Number of cross-validation folds\n",
    "EPOCH = 1  # Training epochs\n",
    "LR = 1e-4  # Learning rate\n",
    "TRAIN_BS = 1 #8  # Training batch size\n",
    "GRAD_ACC_NUM = 8 #1  # Gradient accumulation steps\n",
    "EVAL_BS = 8  # Evaluation batch size\n",
    "FOLD = 0  # Current fold to train\n",
    "SEED = 42  # Random seed for reproducibility\n",
    "\n",
    "# Derive experiment name and paths\n",
    "EXP_ID = \"jigsaw-lora-finetune-baseline\"\n",
    "if IS_DEBUG:\n",
    "    EXP_ID += \"_debug\"\n",
    "EXP_NAME = EXP_ID + f\"_fold{FOLD}\"\n",
    "COMPETITION_NAME = \"jigsaw-kaggle\"\n",
    "OUTPUT_DIR = \"./ \" # f\"/kaggle/output/{EXP_NAME}/\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "MODEL_OUTPUT_PATH = f\"{OUTPUT_DIR}/trained_model\"\n",
    "\n",
    "TRAIN_PATH = \"data/train.csv\" if ENV == \"local\" else \"/kaggle/input/jigsaw-agile-community-rules/train.csv\"\n",
    "TEST_PATH = \"data/test.csv\" if ENV == \"local\" else \"/kaggle/input/jigsaw-agile-community-rules/test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376852b5",
   "metadata": {},
   "source": [
    "### 3. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6c8bee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(TRAIN_PATH)\n",
    "if IS_DEBUG:\n",
    "    # Use a small subset for debugging\n",
    "    df = df.sample(50, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.padding_side = \"left\"  # Important for causal language models\n",
    "\n",
    "# Define system prompt for the classification task\n",
    "SYS_PROMPT = \"\"\"\n",
    "You are given a comment on reddit. Your task is to classify if it violates the given rule. Only respond Yes/No.\n",
    "\"\"\"\n",
    "\n",
    "prompts = []\n",
    "for i, row in df.iterrows():\n",
    "    text = f\"\"\"\n",
    "r/{row.subreddit}\n",
    "Rule: {row.rule}\n",
    "\n",
    "1) {row.positive_example_1}\n",
    "Violation: Yes\n",
    "\n",
    "2) {row.negative_example_1}\n",
    "Violation: No\n",
    "\n",
    "3) {row.negative_example_2}\n",
    "Violation: No\n",
    "\n",
    "4) {row.positive_example_2}\n",
    "Violation: Yes\n",
    "\n",
    "5) {row.body}\n",
    "\"\"\"\n",
    "    \n",
    "    # Format as a chat conversation using the model's template\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": text}\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    ) + \"Answer:\"\n",
    "    prompts.append(prompt)\n",
    "\n",
    "# Add the formatted prompts to the dataframe\n",
    "df[\"text\"] = prompts\n",
    "df[\"label\"] = df[\"rule_violation\"].apply(lambda x: \"Yes\" if x == 1 else \"No\")\n",
    "\n",
    "# Append the label to create completion-based training examples\n",
    "df[\"text\"] = df[\"text\"] + df[\"label\"]\n",
    "\n",
    "# Tokenize the examples\n",
    "def preprocess_row(row, tokenizer) -> dict:\n",
    "    item = tokenizer(row[\"text\"], add_special_tokens=False, truncation=False)\n",
    "    return item\n",
    "\n",
    "def preprocess_df(df, tokenizer) -> pd.DataFrame:\n",
    "    items = []\n",
    "    for _, row in df.iterrows():\n",
    "        items.append(preprocess_row(row, tokenizer))\n",
    "    df = pd.concat([\n",
    "        df,\n",
    "        pd.DataFrame(items)\n",
    "    ], axis=1)\n",
    "    return df\n",
    "\n",
    "df = preprocess_df(df, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996fc48c",
   "metadata": {},
   "source": [
    "### 4. Dataset and Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9355ae1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Loading a GPTQ quantized model requires gptqmodel (`pip install gptqmodel`) or auto-gptq (`pip install auto-gptq`) library. ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m data_collator = DataCollatorForCompletionOnlyLM(\u001b[33m\"\u001b[39m\u001b[33mAnswer:\u001b[39m\u001b[33m\"\u001b[39m, tokenizer=tokenizer)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Load the base model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# device_map=\"auto\",  # Automatically distribute model across available GPUs\u001b[39;49;00m\n\u001b[32m     29\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Configure LoRA parameters\u001b[39;00m\n\u001b[32m     32\u001b[39m lora_config = LoraConfig(\n\u001b[32m     33\u001b[39m     r=\u001b[32m16\u001b[39m,  \u001b[38;5;66;03m# Rank of the update matrices\u001b[39;00m\n\u001b[32m     34\u001b[39m     lora_alpha=\u001b[32m16\u001b[39m,  \u001b[38;5;66;03m# Alpha parameter for LoRA scaling\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     47\u001b[39m     ]\n\u001b[32m     48\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Kaggle\\rules-classification\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:600\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    598\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    599\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    602\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    604\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    605\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    606\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Kaggle\\rules-classification\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:316\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    318\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Kaggle\\rules-classification\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:4879\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4876\u001b[39m     hf_quantizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4878\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4879\u001b[39m     \u001b[43mhf_quantizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4880\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4881\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4882\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4883\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4884\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4885\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4886\u001b[39m     torch_dtype = hf_quantizer.update_torch_dtype(torch_dtype)\n\u001b[32m   4887\u001b[39m     device_map = hf_quantizer.update_device_map(device_map)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Kaggle\\rules-classification\\.venv\\Lib\\site-packages\\transformers\\quantizers\\quantizer_gptq.py:67\u001b[39m, in \u001b[36mGptqHfQuantizer.validate_environment\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mGPU is required to quantize or run quantize model.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_auto_gptq_available() \u001b[38;5;129;01mor\u001b[39;00m is_gptqmodel_available()):\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     68\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mLoading a GPTQ quantized model requires gptqmodel (`pip install gptqmodel`) or auto-gptq (`pip install auto-gptq`) library. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     69\u001b[39m     )\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_auto_gptq_available() \u001b[38;5;129;01mand\u001b[39;00m version.parse(importlib.metadata.version(\u001b[33m\"\u001b[39m\u001b[33mauto_gptq\u001b[39m\u001b[33m\"\u001b[39m)) < version.parse(\n\u001b[32m     71\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m0.4.2\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     72\u001b[39m ):\n\u001b[32m     73\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     74\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou need a version of auto_gptq >= 0.4.2 to use GPTQ: `pip install --upgrade auto-gptq` or use gptqmodel by `pip install gptqmodel>=1.4.3`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     75\u001b[39m     )\n",
      "\u001b[31mImportError\u001b[39m: Loading a GPTQ quantized model requires gptqmodel (`pip install gptqmodel`) or auto-gptq (`pip install auto-gptq`) library. "
     ]
    }
   ],
   "source": [
    "# Create a PyTorch dataset class\n",
    "class ClassifyDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "    ):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index) -> dict:\n",
    "        row = self.df.iloc[index]\n",
    "\n",
    "        inputs = {\n",
    "            \"input_ids\": row[\"input_ids\"],\n",
    "        }\n",
    "        return inputs\n",
    "\n",
    "# Data collator for completion-only learning\n",
    "data_collator = DataCollatorForCompletionOnlyLM(\"Answer:\", tokenizer=tokenizer)\n",
    "\n",
    "# Load the base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    # device_map=\"auto\",  # Automatically distribute model across available GPUs\n",
    ")\n",
    "\n",
    "# Configure LoRA parameters\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Rank of the update matrices\n",
    "    lora_alpha=16,  # Alpha parameter for LoRA scaling\n",
    "    lora_dropout=0.05,  # Dropout probability for LoRA layers\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    bias='none',  # Don't train bias terms\n",
    "    # Target the attention and MLP modules of the transformer\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()  # Show what percentage of parameters will be trained\n",
    "\n",
    "# Initialize Weights & Biases for experiment tracking\n",
    "if WANDB:\n",
    "    wandb.login()\n",
    "    wandb.init(project=COMPETITION_NAME, name=EXP_NAME)\n",
    "    REPORT_TO = \"wandb\"\n",
    "else:\n",
    "    REPORT_TO = \"none\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93ca564",
   "metadata": {},
   "source": [
    "### 5. Cross-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729e4290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and validation sets\n",
    "kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(df)):\n",
    "    if fold == FOLD:\n",
    "        df_train = df.iloc[train_idx].reset_index(drop=True)\n",
    "        df_val = df.iloc[val_idx].reset_index(drop=True)\n",
    "        break\n",
    "\n",
    "# Save the split data\n",
    "df_train.to_pickle(f\"{OUTPUT_DIR}/train.pkl\")\n",
    "df_val.to_pickle(f\"{OUTPUT_DIR}/val.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaa5772",
   "metadata": {},
   "source": [
    "### 6. Training Configuration and Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51636b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36/1408684964.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 05:23, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_OUTPUT_PATH,\n",
    "    logging_steps=10,  # Log metrics every 10 steps\n",
    "    logging_strategy=\"steps\",\n",
    "    eval_strategy=\"no\",  # No evaluation during training\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=0.1,  # Save checkpoint after 10% of training steps\n",
    "    save_total_limit=10,  # Keep only the 10 most recent checkpoints\n",
    "    num_train_epochs=EPOCH,\n",
    "    optim=\"paged_adamw_8bit\",  # 8-bit optimizer for memory efficiency\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_ratio=0.1,  # Warm up learning rate over 10% of steps\n",
    "    learning_rate=LR,\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    # Use BF16 if available, otherwise FP16\n",
    "    bf16=is_torch_bf16_gpu_available(),\n",
    "    fp16=not is_torch_bf16_gpu_available(),\n",
    "\n",
    "    per_device_train_batch_size=TRAIN_BS,\n",
    "    per_device_eval_batch_size=EVAL_BS,\n",
    "    gradient_accumulation_steps=GRAD_ACC_NUM,\n",
    "    gradient_checkpointing=True,  # Save memory with gradient checkpointing\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    group_by_length=False,\n",
    "    report_to=REPORT_TO,\n",
    "    seed=42,\n",
    "    remove_unused_columns=False,  # Keep all columns in the dataset\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=ClassifyDataset(df_train),\n",
    "    eval_dataset=ClassifyDataset(df_val),\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer_output = trainer.train()\n",
    "\n",
    "# Save the final model\n",
    "trainer.save_model(MODEL_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c55027",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "This notebook demonstrates a complete workflow for fine-tuning Qwen2.5 using LoRA for a text classification task. The key components include:\n",
    "\n",
    "1. Setting up the model with quantization (GPTQ-Int4)\n",
    "2. Formatting the data as completion tasks\n",
    "3. Using LoRA to efficiently fine-tune only a small subset of parameters\n",
    "4. Training with mixed precision for memory efficiency\n",
    "\n",
    "After training, the model will be saved and can be used for inference on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01e06fa",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
