{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fc5cbc3",
   "metadata": {},
   "source": [
    "## Qwen2.5-LoRA-Finetune-Baseline-Train\n",
    "This notebook demonstrates how to fine-tune a large language model (Qwen2.5) using the LoRA (Low-Rank Adaptation) technique for classification tasks. The example uses the Jigsaw dataset to classify if Reddit comments violate community rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42538eac",
   "metadata": {},
   "source": [
    "### Note\n",
    "The training code is for demonstration purposes only. To reproduce the weights in the inference notebook, you need to change the config as follows. Training was performed on a local A6000.\n",
    "\n",
    "- `IS_DEBUG = True` → `False`\n",
    "- `MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4\"` → `\"Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4\"`\n",
    "- `TRAIN_BS = 1` → `8`\n",
    "- `GRAD_ACC_NUM = 8` → `1`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd93151",
   "metadata": {},
   "source": [
    "### References\n",
    "* https://www.kaggle.com/code/abdmental01/jigsaw-mpnet-base-v2-inference-cv-0-876\n",
    "* https://www.kaggle.com/code/aerdem4/jigsaw-acrc-qwen7b-finetune-logits-processor-zoo\n",
    "* https://www.guruguru.science/competitions/24/discussions/21027ff1-2074-4e21-a249-b2d4170bd516/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ccc56a",
   "metadata": {},
   "source": [
    "### 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9645491c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Kaggle\\rules-classification\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "import wandb\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers.utils import is_torch_bf16_gpu_available\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from trl import DataCollatorForCompletionOnlyLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce16c5d4",
   "metadata": {},
   "source": [
    "### 2. Configuration Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d922511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main configuration parameters\n",
    "WANDB = False  # Enable/disable Weights & Biases logging\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4\"  # Pre-trained model to fine-tune\n",
    "IS_DEBUG = True  # Debug mode with small dataset\n",
    "N_FOLDS = 5  # Number of cross-validation folds\n",
    "EPOCH = 1  # Training epochs\n",
    "LR = 1e-4  # Learning rate\n",
    "TRAIN_BS = 1 #8  # Training batch size\n",
    "GRAD_ACC_NUM = 8 #1  # Gradient accumulation steps\n",
    "EVAL_BS = 8  # Evaluation batch size\n",
    "FOLD = 0  # Current fold to train\n",
    "SEED = 42  # Random seed for reproducibility\n",
    "\n",
    "# Derive experiment name and paths\n",
    "EXP_ID = \"jigsaw-lora-finetune-baseline\"\n",
    "if IS_DEBUG:\n",
    "    EXP_ID += \"_debug\"\n",
    "EXP_NAME = EXP_ID + f\"_fold{FOLD}\"\n",
    "COMPETITION_NAME = \"jigsaw-kaggle\"\n",
    "OUTPUT_DIR = \"./ \" # f\"/kaggle/output/{EXP_NAME}/\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "MODEL_OUTPUT_PATH = f\"{OUTPUT_DIR}/trained_model\"\n",
    "\n",
    "TRAIN_PATH = \"data/train.csv\"\n",
    "TEST_PATH = \"data/test.csv\"\n",
    "# TRAIN_PATH = \"/kaggle/input/jigsaw-agile-community-rules/train.csv\"\n",
    "# TEST_PATH = \"/kaggle/input/jigsaw-agile-community-rules/test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376852b5",
   "metadata": {},
   "source": [
    "### 3. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c8bee9",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/jigsaw-agile-community-rules/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/kaggle/input/jigsaw-agile-community-rules/train.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m IS_DEBUG:\n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# Use a small subset for debugging\u001b[39;00m\n\u001b[32m      5\u001b[39m     df = df.sample(\u001b[32m50\u001b[39m, random_state=SEED).reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Kaggle\\rules-classification\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Kaggle\\rules-classification\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Kaggle\\rules-classification\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Kaggle\\rules-classification\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Kaggle\\rules-classification\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/kaggle/input/jigsaw-agile-community-rules/train.csv'"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(TRAIN_PATH)\n",
    "if IS_DEBUG:\n",
    "    # Use a small subset for debugging\n",
    "    df = df.sample(50, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.padding_side = \"left\"  # Important for causal language models\n",
    "\n",
    "# Define system prompt for the classification task\n",
    "SYS_PROMPT = \"\"\"\n",
    "You are given a comment on reddit. Your task is to classify if it violates the given rule. Only respond Yes/No.\n",
    "\"\"\"\n",
    "\n",
    "prompts = []\n",
    "for i, row in df.iterrows():\n",
    "    text = f\"\"\"\n",
    "r/{row.subreddit}\n",
    "Rule: {row.rule}\n",
    "\n",
    "1) {row.positive_example_1}\n",
    "Violation: Yes\n",
    "\n",
    "2) {row.negative_example_1}\n",
    "Violation: No\n",
    "\n",
    "3) {row.negative_example_2}\n",
    "Violation: No\n",
    "\n",
    "4) {row.positive_example_2}\n",
    "Violation: Yes\n",
    "\n",
    "5) {row.body}\n",
    "\"\"\"\n",
    "    \n",
    "    # Format as a chat conversation using the model's template\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": text}\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    ) + \"Answer:\"\n",
    "    prompts.append(prompt)\n",
    "\n",
    "# Add the formatted prompts to the dataframe\n",
    "df[\"text\"] = prompts\n",
    "df[\"label\"] = df[\"rule_violation\"].apply(lambda x: \"Yes\" if x == 1 else \"No\")\n",
    "\n",
    "# Append the label to create completion-based training examples\n",
    "df[\"text\"] = df[\"text\"] + df[\"label\"]\n",
    "\n",
    "# Tokenize the examples\n",
    "def preprocess_row(row, tokenizer) -> dict:\n",
    "    item = tokenizer(row[\"text\"], add_special_tokens=False, truncation=False)\n",
    "    return item\n",
    "\n",
    "def preprocess_df(df, tokenizer) -> pd.DataFrame:\n",
    "    items = []\n",
    "    for _, row in df.iterrows():\n",
    "        items.append(preprocess_row(row, tokenizer))\n",
    "    df = pd.concat([\n",
    "        df,\n",
    "        pd.DataFrame(items)\n",
    "    ], axis=1)\n",
    "    return df\n",
    "\n",
    "df = preprocess_df(df, tokenizer)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996fc48c",
   "metadata": {},
   "source": [
    "### 4. Dataset and Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9355ae1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb516034b714c11b24d7893f6b0caff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float16)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27ca33179b4b4bf186efe04ae5f42a5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86721b9bf49f4c82b3757e6e3e8c7a51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5a509b19d2f41208522617ab159d916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.58G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46b25f6e518c4e76a76bc7ce4c1f6b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8486fe8d90f04054adfed4f2aca4e8d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f9d526bfb134c4d882520d23be72838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 40,370,176 || all params: 1,130,569,216 || trainable%: 3.5708\n"
     ]
    }
   ],
   "source": [
    "# Create a PyTorch dataset class\n",
    "class ClassifyDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "    ):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index) -> dict:\n",
    "        row = self.df.iloc[index]\n",
    "\n",
    "        inputs = {\n",
    "            \"input_ids\": row[\"input_ids\"],\n",
    "        }\n",
    "        return inputs\n",
    "\n",
    "# Data collator for completion-only learning\n",
    "data_collator = DataCollatorForCompletionOnlyLM(\"Answer:\", tokenizer=tokenizer)\n",
    "\n",
    "# Load the base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    # device_map=\"auto\",  # Automatically distribute model across available GPUs\n",
    ")\n",
    "\n",
    "# Configure LoRA parameters\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Rank of the update matrices\n",
    "    lora_alpha=16,  # Alpha parameter for LoRA scaling\n",
    "    lora_dropout=0.05,  # Dropout probability for LoRA layers\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    bias='none',  # Don't train bias terms\n",
    "    # Target the attention and MLP modules of the transformer\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()  # Show what percentage of parameters will be trained\n",
    "\n",
    "# Initialize Weights & Biases for experiment tracking\n",
    "if WANDB:\n",
    "    wandb.login()\n",
    "    wandb.init(project=COMPETITION_NAME, name=EXP_NAME)\n",
    "    REPORT_TO = \"wandb\"\n",
    "else:\n",
    "    REPORT_TO = \"none\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93ca564",
   "metadata": {},
   "source": [
    "### 5. Cross-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729e4290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and validation sets\n",
    "kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(df)):\n",
    "    if fold == FOLD:\n",
    "        df_train = df.iloc[train_idx].reset_index(drop=True)\n",
    "        df_val = df.iloc[val_idx].reset_index(drop=True)\n",
    "        break\n",
    "\n",
    "# Save the split data\n",
    "df_train.to_pickle(f\"{OUTPUT_DIR}/train.pkl\")\n",
    "df_val.to_pickle(f\"{OUTPUT_DIR}/val.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaa5772",
   "metadata": {},
   "source": [
    "### 6. Training Configuration and Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51636b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36/1408684964.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 05:23, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_OUTPUT_PATH,\n",
    "    logging_steps=10,  # Log metrics every 10 steps\n",
    "    logging_strategy=\"steps\",\n",
    "    eval_strategy=\"no\",  # No evaluation during training\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=0.1,  # Save checkpoint after 10% of training steps\n",
    "    save_total_limit=10,  # Keep only the 10 most recent checkpoints\n",
    "    num_train_epochs=EPOCH,\n",
    "    optim=\"paged_adamw_8bit\",  # 8-bit optimizer for memory efficiency\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_ratio=0.1,  # Warm up learning rate over 10% of steps\n",
    "    learning_rate=LR,\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    # Use BF16 if available, otherwise FP16\n",
    "    bf16=is_torch_bf16_gpu_available(),\n",
    "    fp16=not is_torch_bf16_gpu_available(),\n",
    "\n",
    "    per_device_train_batch_size=TRAIN_BS,\n",
    "    per_device_eval_batch_size=EVAL_BS,\n",
    "    gradient_accumulation_steps=GRAD_ACC_NUM,\n",
    "    gradient_checkpointing=True,  # Save memory with gradient checkpointing\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    group_by_length=False,\n",
    "    report_to=REPORT_TO,\n",
    "    seed=42,\n",
    "    remove_unused_columns=False,  # Keep all columns in the dataset\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=ClassifyDataset(df_train),\n",
    "    eval_dataset=ClassifyDataset(df_val),\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer_output = trainer.train()\n",
    "\n",
    "# Save the final model\n",
    "trainer.save_model(MODEL_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c55027",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "This notebook demonstrates a complete workflow for fine-tuning Qwen2.5 using LoRA for a text classification task. The key components include:\n",
    "\n",
    "1. Setting up the model with quantization (GPTQ-Int4)\n",
    "2. Formatting the data as completion tasks\n",
    "3. Using LoRA to efficiently fine-tune only a small subset of parameters\n",
    "4. Training with mixed precision for memory efficiency\n",
    "\n",
    "After training, the model will be saved and can be used for inference on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01e06fa",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
